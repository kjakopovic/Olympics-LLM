{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\compilers\\python\\lib\\site-packages (from accelerate) (0.26.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\compilers\\python\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dini\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\dini\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\compilers\\python\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\compilers\\python\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\compilers\\python\\lib\\site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\compilers\\python\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\compilers\\python\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\compilers\\python\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\compilers\\python\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\compilers\\python\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\compilers\\python\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\compilers\\python\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\compilers\\python\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\compilers\\python\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\compilers\\python\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\compilers\\python\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dini\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\compilers\\python\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\compilers\\python\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\compilers\\python\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\compilers\\python\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\compilers\\python\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\compilers\\python\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install accelerate # charset-normalizer  # pandas python-dotenv transformers\n",
    "%pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the most common eaten foodstuff in USA? What is the most popular food in the world? What is the most popular food in the world? What is the most popular food in the world? What is the most popular food in the'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "# response = pipe(\"What is the most common eaten foodstuff in USA?\", return_full_text=False, truncation=True)\n",
    "response = pipe(\"What is the most common eaten foodstuff in USA?\", truncation=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: None\n"
     ]
    }
   ],
   "source": [
    "from charset_normalizer import detect\n",
    "\n",
    "# Read a sample of the file\n",
    "with open('data/Foodex1.csv', 'rb') as file:\n",
    "    raw_data = file.read()\n",
    "\n",
    "# Detect encoding\n",
    "result = detect(raw_data)\n",
    "print(f\"Detected encoding: {result['encoding']}\")\n",
    "\n",
    "encoding = result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAOSTAT script\n",
    "def get_most_eaten_food_in_faostat_dataset():\n",
    "    faostat = pd.read_csv(\"data/FAOSTAT_food_consumption.csv\")\n",
    "    # print(faostat['Item'].unique())\n",
    "    # faostat[faostat['Area'] == 'Afghanistan'].head(50)\n",
    "\n",
    "    faostat_filtered_units = faostat[faostat['Unit'] == '1000 t']\n",
    "\n",
    "    faostat_filtered_units = faostat_filtered_units[faostat_filtered_units['Value'] > 0]\n",
    "\n",
    "    unique_countries = faostat_filtered_units['Area'].unique()\n",
    "\n",
    "    for country in unique_countries:\n",
    "        country_data = faostat_filtered_units[faostat_filtered_units['Area'] == country]\n",
    "\n",
    "        most_eaten_food = country_data[country_data['Value'] == country_data['Value'].max()]['Item'].values[0]\n",
    "\n",
    "        print(f\"In {country} the most eaten food is {most_eaten_food}\")\n",
    "\n",
    "# Dishes script\n",
    "def get_most_eaten_food_in_dishes_dataset():\n",
    "    sentences = []\n",
    "\n",
    "    dish = pd.read_csv(\"data/dishes.csv\")\n",
    "\n",
    "    unique_countries = dish['countries'].unique()\n",
    "\n",
    "    for country in unique_countries:\n",
    "        country_data = dish[dish['countries'] == country]\n",
    "\n",
    "        country_regions = country_data['regions'].unique()\n",
    "\n",
    "        for region in country_regions:\n",
    "            region_data = country_data[country_data['regions'] == region]\n",
    "\n",
    "            if len(region_data['english_name'].values) == 0:\n",
    "                continue\n",
    "            \n",
    "            sentences.append(f\"In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "            # print(f\"In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for text generation\n",
    "class TextGenerationDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # Labels are the same as input_ids for causal language modeling\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"]\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dini\\AppData\\Local\\Temp\\ipykernel_11304\\1965568502.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  0%|          | 0/3 [07:52<?, ?it/s]\n",
      "100%|██████████| 6/6 [00:45<00:00,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 45.5253, 'train_samples_per_second': 0.264, 'train_steps_per_second': 0.132, 'train_loss': 4.899595578511556, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=4.899595578511556, metrics={'train_runtime': 45.5253, 'train_samples_per_second': 0.264, 'train_steps_per_second': 0.132, 'total_flos': 3135504384000.0, 'train_loss': 4.899595578511556, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained text-generation model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Ensure padding tokens are set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Loading dataset\n",
    "texts = get_most_eaten_food_in_dishes_dataset()\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = TextGenerationDataset(texts, tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer\",               # Output directory\n",
    "    learning_rate=5e-5,                 # Learning rate\n",
    "    per_device_train_batch_size=2,      # Batch size\n",
    "    weight_decay=0.01,                  # Weight decay\n",
    "    save_steps=10,                      # Save checkpoint every 10 steps\n",
    "    logging_dir=\"logs\",                 # Log directory\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    fp16=True,                          # Enable mixed precision (if supported)\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trainer\\\\tokenizer_config.json',\n",
       " 'trainer\\\\special_tokens_map.json',\n",
       " 'trainer\\\\vocab.json',\n",
       " 'trainer\\\\merges.txt',\n",
       " 'trainer\\\\added_tokens.json',\n",
       " 'trainer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()  # Saves the model (PyTorch model weights)\n",
    "tokenizer.save_pretrained(training_args.output_dir)  # Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_length=50\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
