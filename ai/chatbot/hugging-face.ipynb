{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.239985Z",
     "start_time": "2025-01-08T07:46:28.234243Z"
    }
   },
   "source": [
    "# %pip install accelerate # charset-normalizer  # pandas python-dotenv transformers\n",
    "# %pip install --upgrade accelerate"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.327944Z",
     "start_time": "2025-01-08T07:46:28.321497Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.417322Z",
     "start_time": "2025-01-08T07:46:28.409452Z"
    }
   },
   "source": [
    "# List all available CUDA devices\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.480449Z",
     "start_time": "2025-01-08T07:46:28.472871Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.650093Z",
     "start_time": "2025-01-08T07:46:28.614396Z"
    }
   },
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "\n",
    "login(token)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "944ecfaa0f374b9e9df2b03af147368c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.708421Z",
     "start_time": "2025-01-08T07:46:28.702871Z"
    }
   },
   "source": [
    "# model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model_id = \"gpt2\"\n",
    "output_dir = \"model/gpt2-food\"\n",
    "enpoint_url = \"../apis/model/gpt-v1\""
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:48:32.959129Z",
     "start_time": "2025-01-08T07:46:28.762299Z"
    }
   },
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_length=50,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# response = pipe(\"What is the most common eaten foodstuff in USA?\", return_full_text=False, truncation=True)\n",
    "response = pipe(\"What is the most eaten food in Algeria?\")\n",
    "response"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4d7ddd6c3f44f1ebe995fc51dd18a6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2e7e81820ff4dcf95efd2cc3253a6f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba4b74f2432f43379ac4f228fbc8ceef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c39027a0546f48ceb38590189826f946"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6076a08f4594695a3acfff95f9d7d39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "561cb0db87af42e1a90a0f7caa7ee2ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4979c1857304749bf31848f4c670030"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the most eaten food in Algeria?\\n\\nThe biggest food are potatoes, and they are often eaten with lamb and chicken. More than 15% of the national people go to Algeria to eat potatoes. However, the food that has the most'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:48:33.683471Z",
     "start_time": "2025-01-08T07:48:32.964841Z"
    }
   },
   "source": [
    "from charset_normalizer import detect\n",
    "\n",
    "# Read a sample of the file\n",
    "with open('data/Foodex1.csv', 'rb') as file:\n",
    "    raw_data = file.read()\n",
    "\n",
    "# Detect encoding\n",
    "result = detect(raw_data)\n",
    "print(f\"Detected encoding: {result['encoding']}\")\n",
    "\n",
    "encoding = result['encoding']"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Foodex1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m detect\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Read a sample of the file\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata/Foodex1.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m      5\u001B[0m     raw_data \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Detect encoding\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Olympics-LLM\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/Foodex1.csv'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:57:24.258425Z",
     "start_time": "2025-01-08T07:57:24.240477Z"
    }
   },
   "source": [
    "# FAOSTAT script\n",
    "def get_most_eaten_food_in_faostat_dataset():\n",
    "    faostat = pd.read_csv(\"data/FAOSTAT_food_consumption.csv\")\n",
    "    # print(faostat['Item'].unique())\n",
    "    # faostat[faostat['Area'] == 'Afghanistan'].head(50)\n",
    "\n",
    "    faostat_filtered_units = faostat[faostat['Unit'] == '1000 t']\n",
    "\n",
    "    faostat_filtered_units = faostat_filtered_units[faostat_filtered_units['Value'] > 0]\n",
    "\n",
    "    unique_countries = faostat_filtered_units['Area'].unique()\n",
    "\n",
    "    for country in unique_countries:\n",
    "        country_data = faostat_filtered_units[faostat_filtered_units['Area'] == country]\n",
    "\n",
    "        most_eaten_food = country_data[country_data['Value'] == country_data['Value'].max()]['Item'].values[0]\n",
    "\n",
    "        print(f\"In {country} the most eaten food is {most_eaten_food}\")\n",
    "\n",
    "# Dishes script\n",
    "def get_most_eaten_food_in_dishes_dataset():\n",
    "    sentences = []\n",
    "\n",
    "    dish = pd.read_csv(\"data/dishes.csv\")\n",
    "    dish['english_name'] = dish['english_name'].fillna(dish['local_name'])\n",
    "\n",
    "    unique_countries = dish['countries'].unique()\n",
    "    list_of_foods = []\n",
    "    for country in unique_countries:\n",
    "        country_data = dish[dish['countries'] == country]\n",
    "\n",
    "        country_regions = country_data['regions'].unique()\n",
    "\n",
    "        for region in country_regions:\n",
    "            region_data = country_data[country_data['regions'] == region]\n",
    "\n",
    "            if len(region_data['english_name'].values) == 0:\n",
    "                continue\n",
    "            \n",
    "            sentences.append(f\"What is the most eaten food in {country}, {region}? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "            sentences.append(f\"What is the most eaten food in {region}? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "            sentences.append(f\"What do people in {region} eat? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "            sentences.append(f\"What do people in {country}, {region} eat? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "            list_of_foods.append(f\"{region_data['english_name'].values[0]}\")\n",
    "            # print(f\"What is the most eaten food in {country}? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "\n",
    "        sentences.append(f\"What is the most eaten food in {country}? The most common eaten foods in {country}: {', '.join(list_of_foods).rstrip()}\")\n",
    "        sentences.append(f\"What is the most eaten food in {country}? The most eaten foods in {country}: {', '.join(list_of_foods).rstrip()}\")\n",
    "        sentences.append(f\"What do people in {country} eat? The most eaten foods in {country}: {', '.join(list_of_foods).rstrip()}\")\n",
    "        list_of_foods = []\n",
    "\n",
    "    return sentences"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:57:27.043258Z",
     "start_time": "2025-01-08T07:57:26.806979Z"
    }
   },
   "source": [
    "# Get some sentences to ask our AI\n",
    "\n",
    "sentences = get_most_eaten_food_in_dishes_dataset()\n",
    "\n",
    "# print(sentences)\n",
    "\n",
    "filtered_items = [item for item in sentences if \"Bosnia and Herzegovina\" in item]\n",
    "\n",
    "print(filtered_items)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the most eaten food in Bosnia and Herzegovina, Balkans? In Bosnia and Herzegovina, Balkans the most eaten food is Cevapi', 'What is the most eaten food in Balkans? In Bosnia and Herzegovina, Balkans the most eaten food is Cevapi', 'What do people in Balkans eat? In Bosnia and Herzegovina, Balkans the most eaten food is Cevapi', 'What do people in Bosnia and Herzegovina, Balkans eat? In Bosnia and Herzegovina, Balkans the most eaten food is Cevapi', 'What is the most eaten food in Bosnia and Herzegovina? The most common eaten foods in Bosnia and Herzegovina: Cevapi', 'What is the most eaten food in Bosnia and Herzegovina? The most eaten foods in Bosnia and Herzegovina: Cevapi', 'What do people in Bosnia and Herzegovina eat? The most eaten foods in Bosnia and Herzegovina: Cevapi']\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Custom Dataset class for text generation\n",
    "class TextGenerationDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, device, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        # Labels are the same as input_ids for causal language modeling\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"]\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_model(model_to_train, tokenizer_for_model, texts, save_model_dir):\n",
    "    # Prepare the dataset\n",
    "    train_dataset = TextGenerationDataset(texts, tokenizer_for_model, device)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"trainer\",               # Output directory\n",
    "        learning_rate=5e-5,                 # Learning rate\n",
    "        per_device_train_batch_size=2,      # Batch size\n",
    "        weight_decay=0.01,                  # Weight decay\n",
    "        save_strategy=\"no\",                 # No saving on checkpoints\n",
    "        logging_dir=\"logs\",                 # Log directory\n",
    "        logging_steps=10,                   # Log every 10 steps\n",
    "        fp16=True,                          # Enable mixed precision (if supported)\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model_to_train,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer_for_model,\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "    model_to_train.save_pretrained(save_model_dir)\n",
    "    tokenizer_for_model.save_pretrained(save_model_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the pretrained text-generation model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Ensure padding tokens are set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Loading dataset\n",
    "texts = get_most_eaten_food_in_dishes_dataset()\n",
    "\n",
    "train_model(model, tokenizer, texts, output_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Retrain the model if it has missing things\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    output_dir\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Ensure padding tokens are set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Loading dataset\n",
    "texts = get_most_eaten_food_in_dishes_dataset()\n",
    "\n",
    "train_model(model, tokenizer, texts, output_dir + \"-1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model2 = AutoModelForCausalLM.from_pretrained(output_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fine-tuned model\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model2,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_length=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "response = classifier(\"What do people in Algeria eat\")\n",
    "response"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Move model to endpoint\n",
    "finished_model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "finished_model.save_pretrained(enpoint_url)\n",
    "tokenizer.save_pretrained(enpoint_url)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
