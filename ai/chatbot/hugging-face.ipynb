{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.239985Z",
     "start_time": "2025-01-08T07:46:28.234243Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install accelerate # charset-normalizer  # pandas python-dotenv transformers\n",
    "# %pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.327944Z",
     "start_time": "2025-01-08T07:46:28.321497Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.417322Z",
     "start_time": "2025-01-08T07:46:28.409452Z"
    }
   },
   "outputs": [],
   "source": [
    "# List all available CUDA devices\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.480449Z",
     "start_time": "2025-01-08T07:46:28.472871Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.650093Z",
     "start_time": "2025-01-08T07:46:28.614396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:46:28.708421Z",
     "start_time": "2025-01-08T07:46:28.702871Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model_id = \"gpt2-medium\"\n",
    "output_dir = \"model/gpt2-medium-food\"\n",
    "enpoint_url = \"../apis/model/gpt-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:48:32.959129Z",
     "start_time": "2025-01-08T07:46:28.762299Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# response = pipe(\"What is the most common eaten foodstuff in USA?\", return_full_text=False, truncation=True)\n",
    "response = pipe(\"What is the most eaten food in Algeria?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:48:33.683471Z",
     "start_time": "2025-01-08T07:48:32.964841Z"
    }
   },
   "outputs": [],
   "source": [
    "from charset_normalizer import detect\n",
    "\n",
    "# Read a sample of the file\n",
    "with open('data/Foodex1.csv', 'rb') as file:\n",
    "    raw_data = file.read()\n",
    "\n",
    "# Detect encoding\n",
    "result = detect(raw_data)\n",
    "print(f\"Detected encoding: {result['encoding']}\")\n",
    "\n",
    "encoding = result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:57:24.258425Z",
     "start_time": "2025-01-08T07:57:24.240477Z"
    }
   },
   "outputs": [],
   "source": [
    "# FAOSTAT script\n",
    "def get_most_eaten_food_in_faostat_dataset():\n",
    "    faostat = pd.read_csv(\"data/FAOSTAT_food_consumption.csv\")\n",
    "    # print(faostat['Item'].unique())\n",
    "    # faostat[faostat['Area'] == 'Afghanistan'].head(50)\n",
    "\n",
    "    faostat_filtered_units = faostat[faostat['Unit'] == '1000 t']\n",
    "\n",
    "    faostat_filtered_units = faostat_filtered_units[faostat_filtered_units['Value'] > 0]\n",
    "\n",
    "    unique_countries = faostat_filtered_units['Area'].unique()\n",
    "\n",
    "    for country in unique_countries:\n",
    "        country_data = faostat_filtered_units[faostat_filtered_units['Area'] == country]\n",
    "\n",
    "        most_eaten_food = country_data[country_data['Value'] == country_data['Value'].max()]['Item'].values[0]\n",
    "\n",
    "        print(f\"In {country} the most eaten food is {most_eaten_food}\")\n",
    "\n",
    "# Dishes script\n",
    "def get_most_eaten_food_in_dishes_dataset():\n",
    "    sentences = []\n",
    "\n",
    "    dish = pd.read_csv(\"data/dishes.csv\")\n",
    "    dish['english_name'] = dish['english_name'].fillna(dish['local_name'])\n",
    "\n",
    "    unique_countries = dish['countries'].unique()\n",
    "    list_of_foods = []\n",
    "    for country in unique_countries:\n",
    "        country_data = dish[dish['countries'] == country]\n",
    "\n",
    "        country_regions = country_data['regions'].unique()\n",
    "\n",
    "        for region in country_regions:\n",
    "            region_data = country_data[country_data['regions'] == region]\n",
    "\n",
    "            if len(region_data['english_name'].values) == 0:\n",
    "                continue\n",
    "            \n",
    "            sentences.append(f\"What is the most eaten food in {country}, {region}? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "            sentences.append(f\"What is the most eaten food in {region}? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "            sentences.append(f\"What do people in {region} eat? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "            sentences.append(f\"What do people in {country}, {region} eat? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "            list_of_foods.append(f\"{region_data['english_name'].values[0]}\")\n",
    "            # print(f\"What is the most eaten food in {country}? In {country}, {region} the most eaten food is {region_data['english_name'].values[0]}\")\n",
    "\n",
    "        sentences.append(f\"What is the most eaten food in {country}? The most common eaten foods in {country}: {', '.join(list_of_foods).rstrip()}\")\n",
    "        sentences.append(f\"What is the most eaten food in {country}? The most eaten foods in {country}: {', '.join(list_of_foods).rstrip()}\")\n",
    "        sentences.append(f\"What do people in {country} eat? The most eaten foods in {country}: {', '.join(list_of_foods).rstrip()}\")\n",
    "        list_of_foods = []\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T07:57:27.043258Z",
     "start_time": "2025-01-08T07:57:26.806979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get some sentences to ask our AI\n",
    "sentences = get_most_eaten_food_in_dishes_dataset()\n",
    "\n",
    "# print(sentences)\n",
    "\n",
    "filtered_items = [item for item in sentences if \"Bosnia and Herzegovina\" in item]\n",
    "\n",
    "print(filtered_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for text generation\n",
    "class TextGenerationDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, device, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        # Labels are the same as input_ids for causal language modeling\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"]\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_to_train, tokenizer_for_model, texts, save_model_dir):\n",
    "    # Prepare the dataset\n",
    "    train_dataset = TextGenerationDataset(texts, tokenizer_for_model, device)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"trainer\",               # Output directory\n",
    "        learning_rate=5e-5,                 # Learning rate\n",
    "        per_device_train_batch_size=2,      # Batch size\n",
    "        weight_decay=0.01,                  # Weight decay\n",
    "        save_strategy=\"no\",                 # No saving on checkpoints\n",
    "        logging_dir=\"logs\",                 # Log directory\n",
    "        logging_steps=10,                   # Log every 10 steps\n",
    "        fp16=True,                          # Enable mixed precision (if supported)\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model_to_train,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer_for_model,\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "    model_to_train.save_pretrained(save_model_dir)\n",
    "    tokenizer_for_model.save_pretrained(save_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained text-generation model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Ensure padding tokens are set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Loading dataset\n",
    "texts = get_most_eaten_food_in_dishes_dataset()\n",
    "\n",
    "train_model(model, tokenizer, texts, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model if it has missing things\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    output_dir\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Ensure padding tokens are set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Loading dataset\n",
    "texts = get_most_eaten_food_in_dishes_dataset()\n",
    "\n",
    "train_model(model, tokenizer, texts, output_dir + \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AutoModelForCausalLM.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model2,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_length=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "response = classifier(\"What is the most eaten food in Brazil?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to endpoint\n",
    "finished_model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "finished_model.save_pretrained(enpoint_url)\n",
    "tokenizer.save_pretrained(enpoint_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
